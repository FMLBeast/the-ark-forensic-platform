# RTX 5000 Ada Instance Configuration
# Instance ID: 21824819
# Host IP: 153.204.80.81

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
GPU_MEMORY_FRACTION=0.9
GPU_MODEL=RTX_5000_ADA
GPU_VRAM=32768

# Ollama Configuration for RTX 5000 Ada (32GB VRAM)
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*
OLLAMA_NUM_PARALLEL=8
OLLAMA_MAX_LOADED_MODELS=4
OLLAMA_FLASH_ATTENTION=1
OLLAMA_GPU_LAYERS=40
OLLAMA_CONTEXT_SIZE=8192
OLLAMA_BATCH_SIZE=512

# Primary models for 32GB VRAM
OLLAMA_PRIMARY_MODEL=codellama:13b-instruct
OLLAMA_SECONDARY_MODEL=llama2:13b-chat
OLLAMA_LIGHTWEIGHT_MODEL=codellama:7b-instruct
OLLAMA_ANALYSIS_MODEL=mistral:7b-instruct

# Instance specific settings
INSTANCE_ID=21824819
INSTANCE_HOST=153.204.80.81
INSTANCE_LOCATION=Japan
INSTANCE_NETWORK_UP=411.5
INSTANCE_NETWORK_DOWN=276.8

# Application configuration
MAX_CONCURRENT_ANALYSIS=16
BATCH_PROCESSING_SIZE=8
ENABLE_GPU_ACCELERATION=true
MEMORY_LIMIT=120GB
DISK_CACHE_SIZE=50GB

# Security and networking
ALLOWED_ORIGINS=*
CORS_ENABLED=true
RATE_LIMIT_REQUESTS=1000
RATE_LIMIT_WINDOW=900

# Monitoring
ENABLE_METRICS=true
ENABLE_GPU_MONITORING=true
LOG_LEVEL=info
METRICS_INTERVAL=30

# Performance tuning
NODE_OPTIONS="--max-old-space-size=16384 --experimental-worker"
NGINX_WORKER_PROCESSES=auto
NGINX_WORKER_CONNECTIONS=2048