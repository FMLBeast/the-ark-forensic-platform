version: '3.8'

# Vast.ai optimized deployment with Ollama LLM orchestration
# Designed for GPU-accelerated inference and forensic analysis

services:
  # Ollama LLM Service (GPU-accelerated for RTX 5000 Ada)
  ollama:
    image: ollama/ollama:latest
    container_name: ark-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama/models:/models
      - ./ollama/init-models.sh:/init-models.sh
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=8
      - OLLAMA_MAX_LOADED_MODELS=4
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_GPU_LAYERS=40
      - OLLAMA_CONTEXT_SIZE=8192
      - OLLAMA_BATCH_SIZE=512
      - OLLAMA_PRIMARY_MODEL=codellama:13b-instruct
      - OLLAMA_SECONDARY_MODEL=llama2:13b-chat
      - OLLAMA_LIGHTWEIGHT_MODEL=codellama:7b-instruct
      - OLLAMA_ANALYSIS_MODEL=mistral:7b-instruct
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - ark-network

  # Backend Service with Ollama Integration
  backend:
    build:
      context: ../backend
      dockerfile: Dockerfile.vast
    container_name: ark-backend
    restart: unless-stopped
    ports:
      - "3001:3001"
    volumes:
      - /root/hunter_server/data:/app/data
      - backend_logs:/app/logs
      - ./config:/app/config
    environment:
      - NODE_ENV=production
      - PORT=3001
      - DATABASE_URL=sqlite:///app/data/forensic.db
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=codellama:13b-instruct
      - OLLAMA_SECONDARY_MODEL=llama2:13b-chat
      - OLLAMA_LIGHTWEIGHT_MODEL=codellama:7b-instruct
      - OLLAMA_ANALYSIS_MODEL=mistral:7b-instruct
      - OLLAMA_TIMEOUT=120000
      - GPU_ACCELERATION=true
      - VAST_DEPLOYMENT=true
      - LOG_LEVEL=info
      - MAX_CONCURRENT_ANALYSIS=16
      - FORENSIC_DB_PATH=/app/data/stego_results.db
      - HUNTER_SERVER_PATH=/root/hunter_server
      - ENABLE_OLLAMA_ORCHESTRATION=true
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ark-network

  # Frontend Service
  frontend:
    build:
      context: ..
      dockerfile: Dockerfile.vast
    container_name: ark-frontend
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    environment:
      - VITE_API_BASE_URL=http://backend:3001
      - VITE_OLLAMA_URL=http://ollama:11434
      - VITE_ENABLE_GPU_FEATURES=true
      - VITE_DEPLOYMENT_MODE=vast
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ark-network

  # Redis for Caching and Session Management
  redis:
    image: redis:7-alpine
    container_name: ark-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ark-network

  # Nginx Reverse Proxy with SSL
  nginx:
    image: nginx:alpine
    container_name: ark-nginx
    restart: unless-stopped
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - frontend
      - backend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ark-network

  # Monitoring and Metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: ark-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - ark-network

  # GPU Monitoring for Vast.ai
  nvidia-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: ark-gpu-monitor
    restart: unless-stopped
    ports:
      - "9445:9445"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ark-network

volumes:
  ollama_data:
    driver: local
  redis_data:
    driver: local
  backend_logs:
    driver: local
  nginx_logs:
    driver: local
  prometheus_data:
    driver: local

networks:
  ark-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16